## bespoke hash function

MDM-255/128/32 = MiMC in Davies-Meyer mode with 255 bit field, 128 rounds, and
32 keys.  Round constants (unspecified in MiMC paper) are generated from
fractional bits of pi for the first round and repeatedly squared for
subsequent.  Round count lowered from 161 to 128 because our limiting attack is
generic collisions; this hash probably does not achieve 2^255 preimage
security.

## motivating the field size

MDM-ish hash functions require fields of odd dimenson and at least 128ish (for
the Feistel version), although the non-Feistel version uses half as many
rounds.

FRI and the multi-function test described here have security terms of the
inverse field size; below 148 will weaken FRI.

FRI, constraint generation, and the Gao-Mateer FFT can all in various ways
leverage subfields near the problem size, and a subfield-rich field is
advantageous.

## multi-polynomial statements

A multi-polynomial statement consists of an affine set of points (constrained
set), a number of variables defined on the constrained set, and a set of
constraint polynomials with limited total degree (2 or 3) in the variables and
affine shifts of variables.  The constraint polynomials may have high degree in
the coordinate and parameters, but must then be expressed as an arithmetic
circuit.

## non-closed shifts

The papers use shifts that do not close the evaluation set; I'm not sure how
verification is supposed to work with that.  Until I figure it out, we can use
unital shifts, which are unfortunately commutative so we need log(n) of them
instead of 2.

## multi-polynomial proofs

A multi-polynomial proof begins with a satisfying assignment for the variables
on the constrained set.  These are then interpolated to produce one polynomial
per variable and evaluated on a (larger, non-overlapping) affine set of points;
for each constraint, an auxilliary polynomial is generated by using the
evaluation results and dividing by a zero-locus polynomial for the constraint
set.  For cubic polynomials it may be needed to split the auxilliary polynomial
into two polynomials of half degree; this can be done using 1 round each of the
inverse and forward aFFT.

A non-succinct proof is valid if the claimed low-degree polynomials are
actually low degree and if evaluating each constraint produces the
corresponding auxilliary polynomial

A proof can be interactively verified; the verifier challenges the prover with
a random linear combination, the prover responds with a combined polynomial,
then a FRI protocol is used to verify the degree of the combined polynomial
while the verifier performs spot checks to check the combined polynomials
against the original polynomial and constraint set.  Zero knowledge can be
added at the cost of one dummy variable and ca. 1000 dummy points in each real
variable.

## proof that it is sufficient to run FRI on the linear combination

Suppose that _f<sub>1</sub>_ &hellip; _f<sub>k</sub>_ are approximate codewords
in some linear code of dimension _z_ and distance _h_ over **F**, and that a
random linear (affine) combination of the *f*s is within distance _&epsilon;_
of a codeword, _&epsilon;_ < _h_/4, with probability _p +
|_**F**_|<sup>-k</sup>_, _p_ > 2/(|_**F**_| - 1).  Then there exists a set
**S** of at most _&epsilon;_ (1 + _k_ / &lfloor;_p_/2 (|**F**|-1)&rfloor;)
codeword positions such that the error of each _f<sub>i</sub>_ is contained in
**S**.

Proof:

Let _&epsilon;&prime;_ &leq; _&epsilon;_ < _h_/4 be the largest distance at
most _&epsilon;_ which is achieved by a linear combination.  Let _O_ be the
coordinate of a linear combination which reaches _&epsilon;&prime;_.  There are
a fraction _p_ of better linear combinations on (|**F**|<sup>k</sup> -
1)/(|**F**| - 1) disjoint lines passing through _O_; a fraction _p_/2 of such
lines must have _p_/2 better linear combinations each.

Let **DO** be the difference between _O_'s combination and the (unique) nearest
codeword; **DO**'s support has size |**DO**| = _&epsilon;&prime;_.

The total volume of these lines is greater than the volume of a hyperplane so
they span the linear space.  Let **B** be a linearly independent subset of
lines with &geq; _p_/2 _&epsilon;_-points (_&epsilon;&prime;_-points) each;
|**B**| = _k_.

Each line _&ell;_ in **B** has at least _q_ = &lfloor;1 +
_p_(|**F**|-1)/2&rfloor; _&epsilon;&prime;_-points; pick one other than _O_ and
call it _P<sub>&ell;</sub>_, defining **DP<sub>&ell;</sub>** as for **DO**.
Every point on _&ell;_ can be obtained as a linear combination of _O_ and
_P<sub>&ell;</sub>_; let **S<sub>&ell;</sub>** be the union of the supports of
**DO** and **DP<sub>&ell;</sub>**, and thus a superset of the error support for
each point on _&ell;_, immediately |**S<sub>&ell;</sub>**| &leq; 2*&epsilon;*.

Each error position in **S<sub>&ell;</sub>** is linear along _&ell;_, does not
vanish identically, and thus vanishes at exactly one point.  Each
_&epsilon;&prime;_-point requires the error to vanish at
|**S<sub>&ell;</sub>**|-_&epsilon;&prime;_ positions, thus by counting
_q_(|**S<sub>&ell;</sub>**|-_&epsilon;&prime;_) &leq; |**S<sub>&ell;</sub>**|,
|**S<sub>&ell;</sub>**| &leq; _&epsilon;&prime; q_/(_q_-1),
|**S<sub>&ell;</sub>**-**DO**|=|**S<sub>&ell;</sub>**|-_&epsilon;&prime;_ &leq;
_&epsilon;&prime;_/(_q_-1).

Let **S** be the union of **S<sub>&ell;</sub>** for each _&ell;_ in **B**.
|**S**-**DO**| &leq; _&epsilon;&prime; k_/(_q_-1) and since **S** bounds the
error of a linearly independent set of points, it bounds the error for the
whole space, including the *f*s. &#x220e;

## contents of a succinct proof

* Merkle root for the interleaved variable assignment functions; root is used
  as a PRF seed to generate multipliers

* Merkel root for the degree-test functions; this root and previous are used in
  PRF to generate test positions and the first PRI challenge

* Nc consistency checks; authentication paths into the first two roots to
  verify that the test function is close to the correct combination of
  assignment functions

* PRI commitment roots for the degree-test function

* Np PRI query paths

## parameters

MACSP parameters:

* P: number of points

* V: variables per point

* Vs: shifted variables per point

* C: constraints per point

These are largely determined by the problem, but some trading between p and the
others is often possible.

Main IP paramters:

* Nc: number of consistency checks between the assignment functions and test
  function

* R: fractional code rate

FRI parameters:

* Eta: interpolation degree, usually 4

* Nr: number of consistency checks between FRI rounds

Fiat-Shamir parameters:

* Merkle tree radix: 32?

* Mixing between coordinate Merkle layers and variable Merkle layers: enables
  prover memory reduction

The basic SNARK parameters are "recurion optimized" in that they seek to
minimize the cost of proving the verification program.

A full parameter optimization is TBD but R=1/8, Nr=Nc=546 suffices for 2^-128
soundness.

## intuition for aFFT and FRI

A polynomial of degree n can be expressed as a polynomial of degree n/k in a
degree k polynomial and a degree < k leftover.  If the short polynomial is a
subspace quotient polynomial, the polynomial can then be evaluated at n/k
points instead of n points, resulting in degree < k polylets to evaluate at the
original n points.  The affine FFT uses a special degree k polynomial which can
be expanded without multiplications; a more general affine FFT would require
extra multiplications, but FRI can stop before that point.

## achieving zero knowledge

A proof reveals information about the satisfying assignment in the following
ways:

* The FRI subprotocol can reveal nonlocal information about the test function.

  Solution: include one uniformly random "mask" function in the linear
combination, so that revealing the entire test function reveals no information.

* The auxilliary functions for constraints may contain nonlocal information.

  Solution: Use separate Merkle trees for auxilliary functions and any
variable functions which might be accessed shifted, such that the auxilliary
functions are only revealed at coordinates where the verifier can already
compute their values.

* The verifier can directly query a variable assignment in the consistency
test.

  Solution: Execute the protocol using a point set not overlapping with the
point set on which the assignment is defined.

* The verifier's queries to the variable functions can reveal nonlocal
algebraic information about the assignment.

  Solution: Include a block of random values in each variable function on a
number of points greater than the number of verifier queries, including shifts.
Note that this increases the degree of each polynomial; in order to stay below
a power of 2 for FRI, a degree-reduction scheme is needed, such as dividing by
an affine subspace polynomial (since they are very sparse, this is likely
faster than a FFT-based approach).

## random gotchas

Affine subspace polynomials are affine, not linear.

## additive FFT memory optimization

We can make the aFFT run in-place by adding stride support to the "taylor
expansion" subroutine.  The corresponding bit reversal is absorbed by a
relabeling of the basis.

Intel designs share a memory channel between roughly eight cores, providing
rougly 3/4 of a field element of memory bandwidth per multiply.  F1, based on
_very_ imprecise ballparking, should support 100 multipliers and 100 bytes per
cycle of memory access at 500MHz, thus 1/30 field element per multiply (and the
multiplication equivalent of 500 2.5GHz Skylake cores).

During aFFT, the active communicating bits move from the top of the polynomial
to the bottom, then jump back to the top, repeating until the last few lines
are exclusively near the top.  A small number of transposes is thus not
adequate although the number of passes over memory can be reduced by a factor
of 9-20 through cache aware operations; the larger reductions require strongly
coordinated, topology-aware multithreading.

Higher-radix versions of the aFFT are possible in more general fields by
expressing the "taylor expansion" using a more general affine subspace
polynomial.  The major catch is that general twisting is no longer possible,
and if the polynomial has coefficients not 0 or 1 a large number of
multiplications are added; for general bases this is only profitable on FPGAs,
although optimizing the basis to use a smaller polynomial can help a great
deal.  This also increases the number of active bits and thus reduces the
amount of window slack; radices larger than the square root of the relevant
cache size are unlikely to be useful for that reason.

For the practically relevant size range of 2^24 - 2^26 elements, a CPU
implementation will require ~12 read/write passes over memory while the FPGA
implementation requires ~4; both are memory bound and GPUs are worth
investigating.

More efficient shifts are possible using a twist, additive shift by 1, twist
again algorithm.  The second twist can be absorbed in a subsequent FFT.
